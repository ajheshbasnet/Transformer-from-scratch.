# -*- coding: utf-8 -*-
"""EncoderForClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/134Wy3ynjtDCsh1qPvfnOBvMGY3J2xcre
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import sentencepiece as spm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from google.colab import drive

# üìå Mount Google Drive
drive.mount('/content/drive')

!git clone https://huggingface.co/datasets/AmaanP314/youtube-comment-sentiment

dataset = pd.read_csv('/content/youtube-comment-sentiment/youtube-comments-sentiment.csv')
dataset = dataset[['VideoTitle', 'CommentText', 'Sentiment']]

dataset = dataset.reset_index(drop=True)

dataset["input"] = dataset.apply(
    lambda row: f"Video-Title: {row['VideoTitle']}.\n Comment: {row['CommentText']}",
    axis=1
)

dataset.drop(columns = ['VideoTitle', 'CommentText'], inplace = True)

encoder = LabelEncoder()

dataset['Sentiment'] = encoder.fit_transform(dataset['Sentiment'])

dataset

"""#**TOKENIZATION**"""

with open("input_sentence.txt", "w", encoding="utf-8") as en_file:
    for sentence in dataset["input"]:
        en_file.write(sentence.strip() + "\n")

spm.SentencePieceTrainer.train(
    input="/content/input_sentence.txt",
    model_prefix="spm_sentence",
    vocab_size=16000,
    model_type="bpe",  # or "unigram"
    character_coverage=1.0
)

dataset.sample(3)

train, test = train_test_split(dataset, test_size=0.09)

tokenizer = spm.SentencePieceProcessor()
tokenizer.load("spm_sentence.model")

tokenizer.encode("Hi I am from Nepal", out_type = int)

def apply_padding(source_ids, max_seq_len):

  if len(source_ids) < max_seq_len:
    return source_ids + [0] * (max_seq_len - len(source_ids))

  return source_ids[:max_seq_len]

def tokenize(dataset):

  source_ids = tokenizer.encode(dataset["input"], out_type = int)

  # applying <sos> and <eos>
  source_ids = [1] + source_ids + [2]

  return{
      "input": apply_padding(source_ids, max_seq_len = 48), "output" : dataset["Sentiment"]
  }

from datasets import Dataset
test = Dataset.from_pandas(test)
test = Dataset.map(tokenize, remove_columns=['Sentiment', 'input'] )

dataset

from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, dataset):  # dataset = HuggingFace dataset
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]  # Access a single row as a dict
        return {
            'input': torch.tensor(item['input'], dtype=torch.long),
            'output': torch.tensor(item['output'], dtype=torch.long),
        }

# Use it like this:
train_dataset = CustomDataset(dataset)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

"""#**TRANSFORMER**"""

class Word_Embeddings(nn.Module):

  def __init__(self, config):

    super().__init__()
    self.embedding_vectors = nn.Embedding(config.vocab_size, config.d_model)
    self.position_vectors = nn.Embedding(config.max_seq_len, config.d_model)


  def forward(self, x):

    B, T = x.shape                                 # [B, T]
    emb_vectors = self.embedding_vectors(x)        # [B, T, C]

    pos = torch.arange(T, device=x.device)         # [T]
    positions = self.position_vectors(pos)         # [T, C]
    positions = positions.unsqueeze(0)             # [B, T, C]

    word_embedding = emb_vectors+positions

    return word_embedding

class MultiHeadAttention(nn.Module):

  def __init__(self, config):
    super().__init__()

    self.n_heads =config.n_heads
    self.q_mat = nn.Linear(config.d_model, config.d_model)
    self.k_mat = nn.Linear(config.d_model, config.d_model)
    self.v_mat = nn.Linear(config.d_model, config.d_model)
    self.out_proj = nn.Linear(config.d_model, config.d_model)

  def forward(self, x):
    B,T,C = x.shape   # [ B, T, C]

    query = self.q_mat(x) #[ B, T, C]
    key   = self.k_mat(x) #[ B, T, C]
    value = self.v_mat(x)

    n_query = query.reshape(B, T, self.n_heads, C//self.n_heads).transpose(2,1)  # [B, T, H, C/H] --> [B, H, T, C/H]
    n_key   = key.reshape(B, T, self.n_heads, C//self.n_heads).transpose(2,1)
    n_value = value.reshape(B, T, self.n_heads, C//self.n_heads).transpose(2,1)

    attn_score = n_query @ n_key.transpose(2,-1)

    norm_attn_score = F.softmax(attn_score, dim = -1)   #[B, T, T]

    emb_vectors = norm_attn_score @ n_value
    emb_vectors = emb_vectors.transpose(1,2)
    emb_vectors = emb_vectors.reshape(B, T, self.n_heads * C // self.n_heads)
    emb_vectors = self.out_proj(emb_vectors)

    return emb_vectors

class FFNN(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.ffnn1 = nn.Linear(config.d_model, 4 * config.d_model)
    self.relu = nn.ReLU()
    self.ffnn2 = nn.Linear(4 * config.d_model, config.d_model)

  def forward(self, x):
    x = self.ffnn1(x)
    x = self.relu(x)
    x = self.ffnn2(x)

    return x

class SingleEncoder(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.layer1 = nn.LayerNorm(config.d_model)
    self.layer2 = nn.LayerNorm(config.d_model)
    self.MHA = MultiHeadAttention(config)
    self.ffnn = FFNN(config)

  def forward(self, x):

    residual1 = x        #[B, T, C]

    x = self.layer1(x)
    x = self.MHA(x)      #[B, T, C]
    x = x + residual1

    residual2 = x
    x = self.layer2(x)
    x = self.ffnn(x)

    x = x + residual2

    return x

class EncoderStack(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.layers = nn.ModuleList(SingleEncoder(config) for _ in range(config.n_blocks))


  def forward(self, x):
    for layer in self.layers:
      out_vec = layer(x)
      x = out_vec

    return x

class ClassificationHead(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.LastFFNN = nn.Linear(config.d_model, config.total_classes)

  def forward(self, x):
    x = x[:, 0, :]                  # [CLS] token
    logits =  self.LastFFNN(x)
    return logits

class CompleteEncoder(nn.Module):

  def __init__(self,config):
    super().__init__()
    self.embeddings = Word_Embeddings(config)
    self.encoder = EncoderStack(config)
    self.classification = ClassificationHead(config)

  def forward(self, x):
    x = self.embeddings(x)
    x = self.encoder(x)                        #[B, T, C]
    classes = self.classification(x)
    return classes                             #[B, C]

class Model_config :
  d_model = 256
  n_heads = 4
  vocab_size = 16000  # Updated vocab_size
  max_seq_len = 64    # Updated max_seq_len to match padding length
  n_blocks = 4
  total_classes = 3   # Updated total_classes
  learning_rate = 1e-4
  epochs = 10


config = Model_config()

"""#**Training Loop**"""

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device} is used")

model = CompleteEncoder(config)  # type: ignore
model = model.to(device)

epochs = config.epochs
criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)

from tqdm import tqdm
import os

# üìÅ Create folder for checkpoints
checkpoint_dir = "/content/drive/MyDrive/classification_checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)

# üîÅ Resume if a checkpoint exists
def load_checkpoint(model, optimizer):
    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(".pth")])
    if checkpoints:
        latest = checkpoints[-1]
        checkpoint_path = os.path.join(checkpoint_dir, latest)
        print(f"‚úÖ Loading checkpoint: {latest}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state'])
        optimizer.load_state_dict(checkpoint['optimizer_state'])
        return checkpoint['epoch'] + 1
    return 0

#  Training loop
start_epoch = load_checkpoint(model, optimizer)
model.train()

for epoch in range(epochs):
    total_loss = 0
    correct = 0
    total_samples = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
        inputs, target = batch['input'].to(device), batch['output'].to(device)

        optimizer.zero_grad()

        logits = model(inputs)                # [B, C] ‚Äî no need to reshape
        loss = criterion(logits, target)      # [B, C], [B] ‚Äî good

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Accuracy
        preds = torch.argmax(logits, dim=-1)  # [B]
        correct += (preds == target).sum().item()
        total_samples += target.size(0)       # just count batch size

    avg_loss = total_loss / len(train_loader)
    accuracy = correct / total_samples * 100

    print(f"\nEpoch {epoch+1} ‚û§ Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%")

    # üíæ Save checkpoint
    checkpoint = {
        'epoch': epoch,
        'model_state': model.state_dict(),
        'optimizer_state': optimizer.state_dict()
    }
    checkpoint_path = os.path.join(checkpoint_dir, f"epoch_{epoch + 1}.pth")
    torch.save(checkpoint, checkpoint_path)
    print(f"üì¶ Checkpoint saved: {checkpoint_path}")

"""#**Evaluation**"""

model.eval()

total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")

from datasets import Dataset
dataset = Dataset.from_pandas(test)
dataset = dataset.map(tokenize, remove_columns=['Sentiment', 'input'])

test_dataset = CustomDataset(dataset)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

epochs_10_states = '/content/drive/MyDrive/classification_checkpoints/epoch_10.pth'

checkpoint = torch.load(epochs_10_states, map_location=device)
model.load_state_dict(checkpoint['model_state'])

"""#**testing-loop**"""

import torch
import torch.nn.functional as F
from tqdm import tqdm

model.eval()  # Set model to evaluation mode
predicted_vals = []
true_vals = []

with torch.no_grad():  # Disable gradient tracking
    for batch in tqdm(test_loader):
        inputs = batch['input'].to(device)
        actual_output = batch['output'].to(device)

        # Forward pass
        logits = model(inputs)  # [B, C]

        # Convert to probabilities (optional for metrics)
        probs = F.softmax(logits, dim=-1)  # [B, C]

        # Get predicted class (index of max prob)
        predicted = torch.argmax(probs, dim=-1)  # [B]

        # Save predictions and actuals
        predicted_vals.extend(predicted.cpu().tolist())
        true_vals.extend(actual_output.cpu().tolist())

# Example: Accuracy
correct = sum(p == t for p, t in zip(predicted_vals, true_vals))
total = len(true_vals)
accuracy = correct / total

print(f"Test Accuracy: {accuracy * 100:.2f}%")